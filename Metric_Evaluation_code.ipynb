{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "rRaprfub4oD-",
        "EpZD1dPs4p6-",
        "00ARlavx6Hcr"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "dataset = pd.read_csv('./100_subset.csv')"
      ],
      "metadata": {
        "id": "ven5Bnwm0yo-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rodPTLm3azYt",
        "outputId": "22298e2b-ef0c-4445-9e57-03c8f9891832"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###BLEU"
      ],
      "metadata": {
        "id": "rRaprfub4oD-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import corpus_bleu, sentence_bleu, SmoothingFunction\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "bleu_scores = []\n",
        "smoothing_function = SmoothingFunction().method2\n",
        "weights = (0.5, 0.5)\n",
        "\n",
        "\n",
        "for y, (ref_text, hyp_text) in enumerate(zip(dataset['Human-written reference'], dataset['GPT3.5'])):\n",
        "\n",
        "    ref_text = str(ref_text)\n",
        "    hyp_text = str(hyp_text)\n",
        "\n",
        "    ref_tokenized = [word_tokenize(ref_text)]\n",
        "    hyp_tokenized = word_tokenize(hyp_text)\n",
        "\n",
        "\n",
        "    print(ref_text)\n",
        "    print(hyp_text)\n",
        "    print(y+1)\n",
        "\n",
        "    bleu_score = sentence_bleu(ref_tokenized, hyp_tokenized, weights=weights, smoothing_function=smoothing_function)\n",
        "\n",
        "    print(bleu_score)\n",
        "    bleu_scores.append(bleu_score)\n",
        "    print(\"-\"*60)\n",
        "\n",
        "average_bleu_score = sum(scores) / len(scores)\n",
        "print(len(scores))\n",
        "print(\"BLEU Score:\", average_bleu_score)"
      ],
      "metadata": {
        "id": "-_LT-u1a0zIy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###GLEU"
      ],
      "metadata": {
        "id": "EpZD1dPs4p6-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.gleu_score import sentence_gleu\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "gleu_scores = []\n",
        "\n",
        "for y, (ref_text, hyp_text) in enumerate(zip(dataset['Human-written reference'], dataset['GPT3.5'])):\n",
        "\n",
        "    ref_text = str(ref_text)\n",
        "    hyp_text = str(hyp_text)\n",
        "\n",
        "    ref_tokenized = [word for word in word_tokenize(ref_text.lower())]\n",
        "    hyp_tokenized = [word for word in word_tokenize(hyp_text.lower())]\n",
        "    print(ref_text)\n",
        "    print(hyp_text)\n",
        "    print(y+1)\n",
        "    gleu_score = sentence_gleu([ref_tokenized], hyp_tokenized)\n",
        "    print(gleu_score)\n",
        "    gleu_scores.append(gleu_score)\n",
        "    print(\"-\"*60)\n",
        "\n",
        "average_gleu_score = sum(scores) / len(scores)\n",
        "print(len(scores))\n",
        "print(\"GLEU_Score:\", average_gleu_score)"
      ],
      "metadata": {
        "id": "tKjY5bjl4ZaW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###f05"
      ],
      "metadata": {
        "id": "00ARlavx6Hcr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "def calculate_f05_score(reference, hypothesis):\n",
        "    # convert reference and hypothesis sentences to lists of words\n",
        "\n",
        "    ref_words = word_tokenize(reference)\n",
        "    hyp_words = word_tokenize(hypothesis)\n",
        "\n",
        "    # create a set of true positives, false positives, and false negatives\n",
        "    tp = set(ref_words) & set(hyp_words)\n",
        "    fp = set(hyp_words) - tp\n",
        "    fn = set(ref_words) - tp\n",
        "\n",
        "    # calculate precision, recall, and f1 score\n",
        "    precision = len(tp) / (len(tp) + len(fp))\n",
        "    recall = len(tp) / (len(tp) + len(fn))\n",
        "    # Calculate F-0.5\n",
        "    beta=0.5\n",
        "    f05_score = float((1 + (beta**2)) * precision * recall) / (((beta**2) * precision) + recall) if precision + recall else 0.0\n",
        "    return f05_score,recall,precision"
      ],
      "metadata": {
        "id": "Gf2BDP4F6Inf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f05_scores = []\n",
        "f05_recall = []\n",
        "f05_precision = []\n",
        "\n",
        "for y, (ref_text, hyp_text) in enumerate(zip(dataset['Human-written reference'], dataset['GPT3.5'])):\n",
        "\n",
        "    ref_text = str(ref_text)\n",
        "    hyp_text = str(hyp_text)\n",
        "\n",
        "\n",
        "    print(ref_text)\n",
        "    print(hyp_text)\n",
        "    print(y+1)\n",
        "\n",
        "    f05_score,recall,precision = calculate_f05_score(ref_text, hyp_text)\n",
        "\n",
        "\n",
        "    print(f05_score)\n",
        "    print(recall)\n",
        "    print(precision)\n",
        "    f05_scores.append(f05_score)\n",
        "    f05_recall.append(recall)\n",
        "    f05_precision.append(precision)\n",
        "    print(\"-\"*60)\n",
        "\n",
        "average_f05_score = sum(f05_scores) / len(f05_scores)\n",
        "average_recall = sum(f05_recall) / len(f05_recall)\n",
        "average_precision = sum(f05_precision) / len(f05_precision)\n",
        "print(len(f05_scores))\n",
        "print(len(f05_recall))\n",
        "print(len(f05_precision))\n",
        "print(\"Score:\", average_f05_score,\"Recall:\",average_recall,\"Precision:\",average_precision)"
      ],
      "metadata": {
        "id": "b88OMKMb6LVr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
